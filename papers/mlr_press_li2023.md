# Transformers as Algorithms: Generalization and Stability in In-context Learning

**Authors**: Li et al. 2023
**Source**: MLR Press / ICML 2023

**URL**: https://proceedings.mlr.press/v202/li23l.html

## Access this paper

[Click here to view on MLR Press](https://proceedings.mlr.press/v202/li23l.html)

## Summary from papers-summary.md

Transformers satisfy PAC learning bounds through algorithmic stability, with excess risk bounded by |R(T) - R̂(T)| ≤ 2L√(log(2/δ)/(2M)) for L-Lipschitz stable models. Rademacher complexity provides sequence-length-independent generalization guarantees.

## Note

This is a proceedings page. The PDF may be available through the MLR Press link.