# Transformers learn to implement preconditioned gradient descent for in-context learning

**Conference**: NeurIPS 2023

**URL**: https://dl.acm.org/doi/10.5555/3666122.3668099

## Access this paper

[Click here to view on ACM Digital Library](https://dl.acm.org/doi/10.5555/3666122.3668099)

## Summary from papers-summary.md

Transformers discover mesa-optimization during training - implementing subsidiary learning algorithms within their forward pass. They construct internal objectives from context examples and solve them near-optimally, with early layers performing iterative preconditioning and later layers executing gradient steps.

## Note

This is a placeholder file. The actual paper requires ACM Digital Library access to download.