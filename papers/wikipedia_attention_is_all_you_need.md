# Attention Is All You Need

**Authors**: Vaswani et al. 2017
**Source**: Wikipedia Article

**URL**: https://en.wikipedia.org/wiki/Attention_Is_All_You_Need

## Access resources

[Wikipedia Article](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need)

[Original Paper on ArXiv](https://arxiv.org/abs/1706.03762)

## Summary from papers-summary.md

The original transformer paper introducing self-attention as the primary mechanism, replacing recurrence and convolutions. While not explicitly about few-shot learning, it established the architectural foundation that would later prove capable of implementing learning algorithms internally. The key innovation was scaled dot-product attention enabling dynamic computation based on input context.

## Note

This link was to the Wikipedia page about the paper. The actual paper is available on ArXiv at https://arxiv.org/abs/1706.03762