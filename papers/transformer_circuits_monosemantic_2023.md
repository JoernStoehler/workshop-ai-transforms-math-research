# Towards Monosemanticity: Decomposing Language Models With Dictionary Learning

**Authors**: Bricken et al. 2023
**Source**: Transformer Circuits

**URL**: https://transformer-circuits.pub/2023/monosemantic-features

## Access this article

[Click here to read on Transformer Circuits](https://transformer-circuits.pub/2023/monosemantic-features)

## Summary from papers-summary.md

Sparse autoencoders extract monosemantic features from superposition, revealing how transformers encode thousands of concepts in lower dimensions. Part of mechanistic interpretability work showing how transformers decompose into interpretable circuits.

## Note

This is an interactive web article with visualizations, not a downloadable PDF.